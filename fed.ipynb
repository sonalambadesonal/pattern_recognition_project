{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(paths, verbose=-1):\n",
    "    '''expects images for each class in seperate dir, \n",
    "    e.g all digits in 0 class in the directory named 0 '''\n",
    "    data = list()\n",
    "    labels = list()\n",
    "    # loop over the input images\n",
    "    for (i, imgpath) in enumerate(paths):\n",
    "        # load the image and extract the class labels\n",
    "        im_gray = cv2.imread(imgpath, cv2.IMREAD_GRAYSCALE)\n",
    "        image = np.array(im_gray).flatten()\n",
    "        label = imgpath.split(os.path.sep)[-2]\n",
    "        # scale the image to [0, 1] and add to list\n",
    "        data.append(image/255)\n",
    "        labels.append(label)\n",
    "        # show an update every `verbose` images\n",
    "        if verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i + 1, len(paths)))\n",
    "    # return a tuple of the data and labels\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 10000/42000\n",
      "[INFO] processed 20000/42000\n",
      "[INFO] processed 30000/42000\n",
      "[INFO] processed 40000/42000\n"
     ]
    }
   ],
   "source": [
    "#declear path to your mnist data folder\n",
    "img_path = 'C:\\\\Users\\\\rajat\\\\Downloads\\\\archive\\\\trainingSet\\\\trainingSet'\n",
    "\n",
    "#get the path list using the path object\n",
    "image_paths = list(paths.list_images(img_path))\n",
    "#apply our function\n",
    "image_list, label_list = load(image_paths, verbose=10000)\n",
    "\n",
    "#binarize the labels\n",
    "lb = LabelBinarizer()\n",
    "print\n",
    "label_list = lb.fit_transform(label_list)\n",
    "\n",
    "#split data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_list, \n",
    "                                                    label_list, \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients(image_list, label_list, num_clients=10, initial='clients'):\n",
    "    ''' return: a dictionary with keys clients' names and value as \n",
    "                data shards - tuple of images and label lists.\n",
    "        args: \n",
    "            image_list: a list of numpy arrays of training images\n",
    "            label_list:a list of binarized labels for each image\n",
    "            num_client: number of fedrated members (clients)\n",
    "            initials: the clients'name prefix, e.g, clients_1 \n",
    "            \n",
    "    '''\n",
    "\n",
    "    #create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
    "    m_names = ['{}_{}'.format(\"managing_node\", i+1) for i in range(5)]\n",
    "    #randomize the data\n",
    "    data = list(zip(image_list, label_list))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    #shard data and place at each client\n",
    "    size = len(data)//num_clients\n",
    "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "    m_shards = [data[i:i + 2*size] for i in range(0,size*num_clients,size*2)]\n",
    "    \n",
    "\n",
    "\n",
    "    #number of clients must equal number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "    c={}\n",
    "    m={}\n",
    "    for i in range(len(client_names)):\n",
    "        c[client_names[i]] = shards[i]\n",
    "\n",
    "    for i in range(len(m_names)):\n",
    "        m[m_names[i]] = m_shards[i]\n",
    "\n",
    "\n",
    "    return (c,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients,managing_nodes = create_clients(X_train, y_train,num_clients=10, initial='client')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.01176471,\n",
      "       0.00784314, 0.        , 0.        , 0.01960784, 0.01568627,\n",
      "       0.        , 0.02352941, 0.02352941, 0.00784314, 0.01568627,\n",
      "       0.01960784, 0.        , 0.        , 0.01568627, 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.01176471, 0.        , 0.00784314, 0.01176471,\n",
      "       0.        , 0.        , 0.        , 0.04313725, 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.03529412,\n",
      "       0.01568627, 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.00392157, 0.        , 0.00784314, 0.05882353,\n",
      "       0.02352941, 0.        , 0.04705882, 0.01960784, 0.03921569,\n",
      "       0.04313725, 0.        , 0.        , 0.        , 0.04313725,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.01568627, 0.        , 0.        ,\n",
      "       0.01176471, 0.        , 0.        , 0.00392157, 0.08235294,\n",
      "       0.        , 0.        , 0.        , 0.02745098, 0.01960784,\n",
      "       0.01568627, 0.01568627, 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.01960784, 0.01960784, 0.        , 0.        ,\n",
      "       0.00392157, 0.        , 0.        , 0.00392157, 0.03921569,\n",
      "       0.03137255, 0.        , 0.        , 0.00784314, 0.04313725,\n",
      "       0.00392157, 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.00392157, 0.        , 0.        , 0.01960784, 0.03921569,\n",
      "       0.03137255, 0.00392157, 0.        , 0.        , 0.00392157,\n",
      "       0.05098039, 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.05490196, 0.        , 0.        , 0.02745098,\n",
      "       0.01568627, 0.0627451 , 0.39607843, 0.79607843, 1.        ,\n",
      "       0.69411765, 0.19215686, 0.        , 0.03137255, 0.01568627,\n",
      "       0.        , 0.01960784, 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.05882353, 0.01960784, 0.        , 0.22352941, 0.7372549 ,\n",
      "       1.        , 1.        , 0.96862745, 1.        , 0.6       ,\n",
      "       0.05098039, 0.        , 0.01568627, 0.        , 0.01176471,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.00784314, 0.00784314,\n",
      "       0.12156863, 0.91764706, 1.        , 0.58431373, 0.14901961,\n",
      "       0.23137255, 0.99215686, 0.70588235, 0.04313725, 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.01960784, 0.        , 0.04705882, 0.78431373, 1.        ,\n",
      "       0.64313725, 0.03921569, 0.        , 0.49019608, 0.95294118,\n",
      "       0.61960784, 0.03529412, 0.        , 0.01960784, 0.        ,\n",
      "       0.00392157, 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.03921569,\n",
      "       0.53333333, 0.96862745, 0.8745098 , 0.02745098, 0.01176471,\n",
      "       0.12156863, 0.96078431, 1.        , 0.21176471, 0.        ,\n",
      "       0.05882353, 0.        , 0.00392157, 0.01176471, 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.01176471, 0.08627451, 0.92941176, 0.99215686,\n",
      "       0.37254902, 0.        , 0.0745098 , 0.69019608, 0.96862745,\n",
      "       1.        , 0.36078431, 0.        , 0.        , 0.10588235,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.28627451, 1.        , 0.75686275, 0.06666667, 0.02745098,\n",
      "       0.55686275, 1.        , 0.96078431, 0.89019608, 0.12941176,\n",
      "       0.04705882, 0.        , 0.        , 0.0627451 , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.02745098, 0.4       , 0.89411765,\n",
      "       0.78039216, 0.4       , 0.77647059, 1.        , 0.9372549 ,\n",
      "       0.94117647, 0.22352941, 0.        , 0.14117647, 0.01176471,\n",
      "       0.        , 0.        , 0.05490196, 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.01568627, 0.01176471, 1.        , 0.97254902, 0.98823529,\n",
      "       1.        , 0.96470588, 1.        , 0.25882353, 0.02745098,\n",
      "       0.01176471, 0.2       , 0.03529412, 0.        , 0.03921569,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.03529412,\n",
      "       0.21176471, 0.69803922, 0.87058824, 0.96470588, 0.98431373,\n",
      "       0.63529412, 0.02745098, 0.        , 0.00392157, 0.        ,\n",
      "       0.00392157, 0.03921569, 0.        , 0.02745098, 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.00784314, 0.03529412, 0.        , 0.04313725,\n",
      "       0.38431373, 1.        , 0.90196078, 0.16078431, 0.        ,\n",
      "       0.05098039, 0.        , 0.05882353, 0.        , 0.02745098,\n",
      "       0.        , 0.00784314, 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.23137255, 1.        , 0.96078431,\n",
      "       0.55686275, 0.        , 0.05098039, 0.        , 0.02352941,\n",
      "       0.        , 0.        , 0.00784314, 0.02352941, 0.00784314,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.01176471, 0.00392157, 0.0627451 ,\n",
      "       0.79607843, 1.        , 0.8627451 , 0.        , 0.01960784,\n",
      "       0.00784314, 0.        , 0.01568627, 0.01568627, 0.        ,\n",
      "       0.        , 0.01568627, 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.5254902 , 0.96862745, 0.89019608,\n",
      "       0.11764706, 0.00784314, 0.01568627, 0.        , 0.05882353,\n",
      "       0.        , 0.05490196, 0.03137255, 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.16470588,\n",
      "       1.        , 0.91764706, 0.39607843, 0.        , 0.00784314,\n",
      "       0.        , 0.00392157, 0.02352941, 0.10588235, 0.15686275,\n",
      "       0.01568627, 0.02352941, 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.0627451 , 0.8745098 , 0.95686275, 0.80392157,\n",
      "       0.        , 0.00784314, 0.05882353, 0.        , 0.        ,\n",
      "       0.04313725, 0.70588235, 0.4       , 0.        , 0.        ,\n",
      "       0.00784314, 0.01176471, 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.48235294,\n",
      "       0.99607843, 0.90588235, 0.16470588, 0.01960784, 0.        ,\n",
      "       0.06666667, 0.        , 0.05098039, 0.47058824, 1.        ,\n",
      "       0.47058824, 0.        , 0.        , 0.01568627, 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.71764706, 0.97254902, 0.32941176,\n",
      "       0.        , 0.01960784, 0.        , 0.01568627, 0.        ,\n",
      "       0.30196078, 1.        , 0.95686275, 0.29411765, 0.        ,\n",
      "       0.02352941, 0.03921569, 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.18039216, 0.25098039, 0.0745098 , 0.        , 0.02745098,\n",
      "       0.        , 0.        , 0.01176471, 0.74117647, 0.98823529,\n",
      "       0.65882353, 0.        , 0.01568627, 0.        , 0.02352941,\n",
      "       0.01568627, 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.08235294,\n",
      "       0.        , 0.        , 0.03137255, 0.        , 0.        ,\n",
      "       0.01568627, 0.68627451, 0.80392157, 0.07843137, 0.        ,\n",
      "       0.        , 0.02745098, 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.01568627, 0.        , 0.        ,\n",
      "       0.03529412, 0.00392157, 0.        , 0.00392157, 0.        ,\n",
      "       0.07843137, 0.        , 0.        , 0.03529412, 0.        ,\n",
      "       0.        , 0.0627451 , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.03921569, 0.        , 0.        , 0.01960784, 0.        ,\n",
      "       0.        , 0.00784314, 0.        , 0.        , 0.09803922,\n",
      "       0.        , 0.05098039, 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        ]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1]))\n"
     ]
    }
   ],
   "source": [
    "print(clients['client_1'][101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(data_shard, bs=32):\n",
    "    '''Takes in a clients data shard and create a tfds object off it\n",
    "    args:\n",
    "        shard: a data, label constituting a client's data shard\n",
    "        bs:batch size\n",
    "    return:\n",
    "        tfds object'''\n",
    "    #seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    return dataset.shuffle(len(label)).batch(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process and batch the training data for each client\n",
    "clients_batched = dict()\n",
    "managing_nodes_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)\n",
    "#process and batch the test set  \n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 784), (None, 10)), types: (tf.float64, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "print(clients_batched['client_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (managing_node_name,data) in managing_nodes.items():\n",
    "    managing_nodes_batched[managing_node_name] = batch_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "managing_node_test=list()\n",
    "for i in range(len(managing_nodes)):\n",
    "    managing_node_test.append(tf.data.Dataset.from_tensor_slices((X_test[i:i+2], y_test[i:i+2])).batch(len(y_test[i:i+2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP:\n",
    "    @staticmethod\n",
    "    def build(shape, classes):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(200, input_shape=(shape,)))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(200))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python39\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr = 0.01 \n",
    "comms_round = 50\n",
    "loss='categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "optimizer = SGD(lr=lr, \n",
    "                decay=lr / comms_round, \n",
    "                momentum=0.9\n",
    "               )          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smlp_global = SimpleMLP()\n",
    "global_model = smlp_global.build(784, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2010      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 199,210\n",
      "Trainable params: 199,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "global_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_scalling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    #get the bs\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    #first calculate the total training data points across clinets\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "    return local_count/global_count\n",
    "\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "\n",
    "def test_model(X_test, Y_test,  model, comm_round):\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    #logits = model.predict(X_test, batch_size=100)\n",
    "    logits = model.predict(X_test)\n",
    "    loss = cce(Y_test, logits)\n",
    "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
    "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model2(X_test, Y_test,  model, comm_round):\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    #logits = model.predict(X_test, batch_size=100)\n",
    "    logits = model.predict(X_test)\n",
    "    loss = cce(Y_test, logits)\n",
    "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
    "    print('comm_round: {} | managing_node_acc: {:.3%} | managing_node_loss: {}'.format(comm_round, acc, loss))\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 1s 2ms/step\n",
      "comm_round: 0 | global_acc: 88.571% | global_loss: 1.6714301109313965\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 1 | global_acc: 90.833% | global_loss: 1.6184245347976685\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 2 | global_acc: 91.976% | global_loss: 1.59931218624115\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 3 | global_acc: 92.333% | global_loss: 1.5892611742019653\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 4 | global_acc: 92.976% | global_loss: 1.580146074295044\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 5 | global_acc: 93.143% | global_loss: 1.5752205848693848\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 6 | global_acc: 93.667% | global_loss: 1.5693778991699219\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 7 | global_acc: 93.548% | global_loss: 1.5659083127975464\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 8 | global_acc: 93.833% | global_loss: 1.562936544418335\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 9 | global_acc: 93.952% | global_loss: 1.5602836608886719\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 10 | global_acc: 94.119% | global_loss: 1.5581257343292236\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 11 | global_acc: 94.167% | global_loss: 1.5566792488098145\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 12 | global_acc: 94.286% | global_loss: 1.5545088052749634\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 13 | global_acc: 94.476% | global_loss: 1.5528830289840698\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 14 | global_acc: 94.548% | global_loss: 1.5516167879104614\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 15 | global_acc: 94.619% | global_loss: 1.5498123168945312\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 16 | global_acc: 94.571% | global_loss: 1.5485541820526123\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 17 | global_acc: 94.619% | global_loss: 1.547737717628479\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 18 | global_acc: 94.619% | global_loss: 1.5467393398284912\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 19 | global_acc: 94.738% | global_loss: 1.545451045036316\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 20 | global_acc: 94.786% | global_loss: 1.5445798635482788\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 21 | global_acc: 94.929% | global_loss: 1.5436877012252808\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 22 | global_acc: 94.810% | global_loss: 1.542715311050415\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 23 | global_acc: 95.024% | global_loss: 1.5422751903533936\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 24 | global_acc: 95.024% | global_loss: 1.541485071182251\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 25 | global_acc: 95.024% | global_loss: 1.5410351753234863\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 26 | global_acc: 95.024% | global_loss: 1.5398812294006348\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 27 | global_acc: 95.071% | global_loss: 1.5395503044128418\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 28 | global_acc: 95.095% | global_loss: 1.538923740386963\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 29 | global_acc: 95.143% | global_loss: 1.5385061502456665\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 30 | global_acc: 95.190% | global_loss: 1.537656307220459\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 31 | global_acc: 95.286% | global_loss: 1.5373170375823975\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 32 | global_acc: 95.262% | global_loss: 1.537042498588562\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 33 | global_acc: 95.143% | global_loss: 1.5364892482757568\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 34 | global_acc: 95.381% | global_loss: 1.5360405445098877\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 35 | global_acc: 95.238% | global_loss: 1.5358624458312988\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 36 | global_acc: 95.286% | global_loss: 1.5349172353744507\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 37 | global_acc: 95.381% | global_loss: 1.5347888469696045\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 38 | global_acc: 95.357% | global_loss: 1.5345433950424194\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 39 | global_acc: 95.405% | global_loss: 1.5340007543563843\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 40 | global_acc: 95.476% | global_loss: 1.5336494445800781\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 41 | global_acc: 95.476% | global_loss: 1.533447265625\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 42 | global_acc: 95.452% | global_loss: 1.5327553749084473\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 43 | global_acc: 95.452% | global_loss: 1.5325188636779785\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 44 | global_acc: 95.571% | global_loss: 1.5323477983474731\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 45 | global_acc: 95.500% | global_loss: 1.5320340394973755\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 46 | global_acc: 95.452% | global_loss: 1.531741976737976\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 47 | global_acc: 95.500% | global_loss: 1.5316252708435059\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 48 | global_acc: 95.595% | global_loss: 1.531127691268921\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 49 | global_acc: 95.595% | global_loss: 1.530874252319336\n"
     ]
    }
   ],
   "source": [
    "smlp_global = SimpleMLP()\n",
    "global_model = smlp_global.build(784, 10)\n",
    "loss=[]\n",
    "acc=[]\n",
    "#commence global training loop\n",
    "for comm_round in range(comms_round):\n",
    "            \n",
    "    # get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    #randomize client data - using keys\n",
    "    client_names= list(clients_batched.keys())\n",
    "    random.shuffle(client_names)\n",
    "    \n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:\n",
    "        smlp_local = SimpleMLP()\n",
    "        local_model = smlp_local.build(784, 10)\n",
    "        local_model.compile(loss='categorical_crossentropy', \n",
    "                      optimizer=optimizer, \n",
    "                      metrics=metrics)\n",
    "        \n",
    "        #set local model weight to the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        #fit local model with client's data\n",
    "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
    "        \n",
    "        #scale the model weights and add to list\n",
    "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "        \n",
    "        #clear session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "        \n",
    "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    \n",
    "    #update global model \n",
    "    global_model.set_weights(average_weights)\n",
    "    \n",
    "    #test global model and print out metrics after each communications round\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)\n",
    "        loss.append(global_loss)\n",
    "        acc.append(global_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model.save(\"F:/SegmentationModel/UNET/model-sdc-seg-v12.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = acc\n",
    "c_round=[]\n",
    "for i in range(50):\n",
    "    c_round.append(i+1)\n",
    "\n",
    "\n",
    "plt.plot(c_round, acc, 'y', label='Training Accuracy')\n",
    "plt.title('Training Accuracy w.r.t Communication rounds')\n",
    "plt.xlabel('Communication rounds')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25034407fed5d681614dac11a1c0537e8cb49e3a8883c071303eea01322943d9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25034407fed5d681614dac11a1c0537e8cb49e3a8883c071303eea01322943d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
